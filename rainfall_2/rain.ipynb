{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gSR_dtIhpLJG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNIFdrd4pLJH",
        "outputId": "28133c63-4bff-4fd1-8e4b-d669323b2951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost not found, installing...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package_name):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Error installing {package_name}\")\n",
        "\n",
        "# Check and install packages\n",
        "try:\n",
        "    import catboost\n",
        "except ImportError:\n",
        "    print(\"CatBoost not found, installing...\")\n",
        "    install_package('catboost')\n",
        "\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError:\n",
        "    print(\"XGBoost not found, installing...\")\n",
        "    install_package('xgboost')\n",
        "\n",
        "try:\n",
        "    import lightgbm\n",
        "except ImportError:\n",
        "    print(\"LightGBM not found, installing...\")\n",
        "    install_package('lightgbm')\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except ImportError:\n",
        "    print(\"Optuna not found, installing...\")\n",
        "    install_package('optuna')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ey-GPObpLJH"
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv('test.csv')\n",
        "x=train.drop(['rainfall','id'],axis=1).copy()\n",
        "y=train['rainfall']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsWXgHuXpLJH"
      },
      "outputs": [],
      "source": [
        "def describe(df):\n",
        "  desc=pd.DataFrame()\n",
        "  desc['dtype']=df.dtypes\n",
        "  desc['null']=df.isnull().sum()\n",
        "  desc['%null'] = desc['null'] / len(df) * 100\n",
        "  # desc['mean']=df.mean()\n",
        "  # desc['median']=df.median()\n",
        "  #desc['%unique'] = desc['nunique'] /len(df)*100\n",
        "  desc = pd.concat([desc,df.describe(include = 'all').T],axis=1).sort_values(by='null',ascending=False)\n",
        "  desc['count']=df.count()\n",
        "  desc['unique']=df.nunique()\n",
        "  # desc['freq/mean']=desc['freq'].fillna(desc['mean']).drop(['freq','mean'],axis=1)\n",
        "  display(desc)\n",
        "# describe(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFRAn1AapLJH"
      },
      "outputs": [],
      "source": [
        "xtr=x[:1752]\n",
        "xte=x[1752:]\n",
        "ytr=y[:1752]\n",
        "yte=y[1752:]\n",
        "\n",
        "# # @title gaussian\n",
        "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "# model=HistGradientBoostingClassifier()\n",
        "\n",
        "# model.fit(xtr,ytr)\n",
        "# model.score(xte,yte)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_UUOPgapLJH"
      },
      "outputs": [],
      "source": [
        "xte.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M28ogtgpLJI"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAZxFH8CpLJI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_auc_score,recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32NwV9yhpLJI"
      },
      "outputs": [],
      "source": [
        "# @title xgboost\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "\n",
        "def create(trial):\n",
        "  params = {\n",
        "          \"min_child_weight\":trial.suggest_int('min_child_weight',1,5),\n",
        "          \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
        "          \"gamma\":trial.suggest_float('gamma',0,0.5),\n",
        "          \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "          \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "          \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "          #\"reg_alpha\":trial.suggest_float('reg_alpha',0 ,5),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "          \"max_bin\" :trial.suggest_int('max_bin',500,8000),\n",
        "          \"scale_pos_weight\":trial.suggest_float('scale_pos_weight',1.0,16.0,),\n",
        "\n",
        "\n",
        "      }\n",
        "\n",
        "  return params\n",
        "\n",
        "model=xgb.XGBClassifier(\n",
        "#      learning_rate =0.1, max_depth=4,\n",
        "#  min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,reg_alpha=0,\n",
        "#  tree_method=\"hist\"\n",
        "eval_metric=\"aucpr\")\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "\n",
        "    model = xgb.XGBClassifier(**create(trial))\n",
        "    model.fit(xtr, ytr)\n",
        "    predictions = model.predict(xte)\n",
        "    acc = f1_score(yte, predictions)\n",
        "    # rmse = -mean_squared_error(yte, predictions)\n",
        "\n",
        "    #cross = cross_val_score(model,x,y,cv=3, n_jobs=-1).mean()\n",
        "    return acc\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "importance1 = optuna.importance.get_param_importances(study)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "\n",
        "best_params_xgb=study.best_params\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdlhLlLYpLJI"
      },
      "outputs": [],
      "source": [
        "# @title LightGBM\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"'force_all_finite' was renamed\")\n",
        "\n",
        "def create(trial):\n",
        "\n",
        "  params = {\n",
        "        \"verbosity\": -1,\n",
        "        # \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1a),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 20),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "        \"random_state\": 42,\n",
        "    }\n",
        "  return params\n",
        "\n",
        "model=lgb.LGBMClassifier(# \"num_class\": 3,  # Set the number of classes\n",
        "    objective='binary',metric='auc_mu')\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "\n",
        "    model = lgb.LGBMClassifier(**create(trial))\n",
        "    model.fit(xtr, ytr)\n",
        "    predictions = model.predict(xte)\n",
        "    acc = f1_score(yte, predictions)\n",
        "    # rmse = -mean_squared_error(yte, predictions)\n",
        "\n",
        "    #cross = cross_val_score(model,x,y,cv=3, n_jobs=-1).mean()\n",
        "    return acc\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "importance2 = optuna.importance.get_param_importances(study)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "best_params_lgb=study.best_params\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVbwxhbwpLJI"
      },
      "outputs": [],
      "source": [
        "# @title catboost\n",
        "from catboost import CatBoostClassifier, Pool, cv, MetricVisualizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "def create(trial):\n",
        "\n",
        "  params = {\n",
        "\n",
        "        \"random_state\": 42,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 0.9),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
        "        \"scale_pos_weight\":trial.suggest_float('scale_pos_weight',1.0,16.0,)\n",
        "    }\n",
        "  return params\n",
        "\n",
        "\n",
        "model=CatBoostClassifier(eval_metric='F1',verbose=100)\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "\n",
        "    model = CatBoostClassifier(**create(trial))\n",
        "    model.fit(xtr, ytr)\n",
        "    predictions = model.predict(xte)\n",
        "    acc = f1_score(yte, predictions)\n",
        "    # rmse = -mean_squared_error(yte, predictions)\n",
        "\n",
        "    #cross = cross_val_score(model,x,y,cv=3, n_jobs=-1).mean()\n",
        "    return acc\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "importance3 = optuna.importance.get_param_importances(study)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "best_params_cat=study.best_params\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "HBJajMfDqYr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkRyvsnZpLJJ"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(**best_params_xgb,eval_metric='aucpr', random_state=42),\n",
        "    'CatBoost': CatBoostClassifier(**best_params_cat,eval_metric='F1', verbose=100,random_state=42),\n",
        "    'LightGBM': lgb.LGBMClassifier(**best_params_lgb,random_state=42,objective='binary'),\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe2L1qpopLJJ"
      },
      "outputs": [],
      "source": [
        "for name, model in models.items():\n",
        "    print(f'\\nTraining {name}...')\n",
        "    model.fit(xtr, ytr)\n",
        "    y_pred = model.predict(xte)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(yte, y_pred)\n",
        "    f1 = f1_score(yte, y_pred)\n",
        "    roc_auc = roc_auc_score(yte, y_pred)\n",
        "    recall=recall_score(yte,y_pred)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'recall: {recall:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "    print(f'ROC-AUC: {roc_auc:.4f}')\n",
        "    print('Classification Report:\\n', classification_report(yte, y_pred))\n",
        "    print('Confusion Matrix:\\n', confusion_matrix(yte, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6FCWxukpLJJ"
      },
      "outputs": [],
      "source": [
        "model=lgb.LGBMClassifier(**best_params_lgb,random_state=42,objective='binary')\n",
        "model.fit(xtr,ytr)\n",
        "model.score(xte,yte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsbiw-bIpLJJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(train.corr(),annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance (Random Forest as an example)\n",
        "feature_importances = pd.Series(models['Random Forest'].feature_importances_, index=x.columns)\n",
        "feature_importances.nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 Important Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ly1tQCEdp614"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtilZUKspLJJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "l=model.predict(test.drop('id',axis=1))\n",
        "df=pd.DataFrame()\n",
        "df['rainfall']=l\n",
        "df['id']=test['id']\n",
        "df.to_csv(\"sample_submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kaggle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}