{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9A+IlUyxYa/O/YkvaLEY2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PioQ-miHNgaw"},"outputs":[],"source":["#The object returned is not a data frame but a TextFileReader which needs to be iterated to get the data.\n","#when using chunksize\n","\n","\n","\n","df = pd.read_csv(\"train/train.csv\", chunksize=10000)\n","\n","for data in df:\n","    pprint(data.shape)\n","\n","itr = next(df.iterrows())[1]\n","itr\n","\n","for index, row in df.iterrows():\n","    print(row['name'], row['age'])\n","\n","for row in df.itertuples():\n","    print(getattr(row, 'name'), getattr(row, 'age'))\n","\n"]},{"cell_type":"code","source":["# @title other libraries\n","\n","\n","import vaex\n","%time df_vaex = vaex.open(\"dataset1.csv.hdf5\")\n","%time df_vaex.count(binby=df_vaex.column7,\n","                    limits=[0, 20], shape=10)\n","\n","import modin.pandas as mpd\n","modin_df = mpd.DataFrame(data)\n","modin_df = mpd.read_csv(\"demo.csv\")"],"metadata":{"id":"M3Dh1O87QpzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title other techniques\n","\n","\n","    # Use Efficient Datatypes: Utilize more memory-efficient data types (e.g., int32 instead of int64, float32 instead of float64) to reduce memory usage.\n","\n","    # Load Less Data: Use the use-cols parameter in pd.read_csv() to load only the necessary columns, reducing memory consumption.\n","\n","    # Sampling: For exploratory data analysis or testing, consider working with a sample of the dataset instead of the entire dataset.\n","\n","    # Chunking: Use the chunksize parameter in pd.read_csv() to read the dataset in smaller chunks, processing each chunk iteratively.\n","\n","    # Optimizing Pandas dtypes: Use the astype method to convert columns to more memory-efficient types after loading the data, if appropriate.\n","\n","    # Parallelizing Pandas with Dask: Use Dask, a parallel computing library, to scale Pandas workflows to larger-than-memory datasets by leveraging parallel processing.\n"],"metadata":{"id":"TT5800dkQ9cw"},"execution_count":null,"outputs":[]}]}