{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRSpR/KJ7Q+G0GBx9eTIVQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AK1wzxDATgRj"},"outputs":[],"source":["from xgboost import XGBClassifier\n","from xgboost import XGBRegressor\n","model=XGBClassifier()\n","model = XGBRegressor(learning_rate =.1, n_estimators=10,\n","                     max_depth=2, min_child_weight=3, gamma=0,\n","                     subsample=.8, colsample_bytree=.7, reg_alpha=1,\n","                     objective= 'reg:linear')\n","from catboost import CatBoostRegressor\n","from lightgbm import LGBMRegressor\n","\n","df.corr()\n","h=corremat.\n","\n","#manual optimization\n","def optim(params,p_names,x,y):\n","  params =dict(zip(pram_names,params))#dont needed this line for hyperopt and p_names also not needed beacaue params is decit\n","  model=ensamble.RandomForestClassifier(**params)\n","  kf=model_selection.StratifiedKFold(n_splits=5)\n","  acc=[]\n","  for idx in kf.split(X=x,y=y):\n","    train_idx,test_idx=idx[0],idx[1]\n","    xtrain=x[train_idx]\n","    ytrain=y[train_idx]\n","\n","    xtest=x[test_idx]\n","    ytest=y[test_idx]\n","\n","    model.fit(xtrain,ytrian)\n","    preds=model.predict(xtest)\n","    fold_acc =metrices.accuracy_score[ytest,preds]\n","    acc.append{fold_acc}\n","  return -1.0*np.mean(acc)#-1 not needed for log losses\n","from functools import partial\n","from skopt import space\n","from skopt import gp_minimize\n","from hyperopt import hp,fmin,tpe,Trials,\n","from hyperopt.pyll.base import scope\n","\n","#convert to dictionory for hyperopt\n","param_space={'max_depth':scope.int(hp.quniform('max_depth',3,13,)),\n","             'n_estimators':scope.int( hp.quniform('n_estimators',100,600)),\n","             'criterion': hp.choice('criterion',['gini','entropy']),\n","            'max_features':  hp.unfiorm('max_features',0.01,1)}\n","trials=Trials()\n","result=fmin{\n","    fn=optim,\n","    spcae=param_space,\n","    algo=tpe.suggest,\n","    max_level=15,\n","    trials=trials\n","}\n","\n","param_space=[space.Integer(3,13,name='max_depth'),\n","             space.Integer(100,600,name='n_estimators'),\n","             space.Catogarical(['gini','entropy'],name='criterion'),\n","             space.Real(0.01,priors='unfiorm',name='max_features')]\n","\n","p_names=['max_depth','n_estimators','critirerion','maz_featuers']\n","optm=partial(\n","    optim ,\n","    p_names=p_names\n","    ,x=x,y=y)\n","result=gp_minmize(optm,dimenisions=param_space,\n","                  n_calls=15,\n","                  n_random_starts=10\n","                  verbose=10)\n","print(dict(zip(p_names,result.x)))\n","\n","\n","\n","#for hyperparameter optimization\n","from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n","\n","models=RandomizedSearchCV(model,param_disturbutions=params,n_iter=5,scoring='roc_auc',n_job=-1,cv=5,verbose=3)\n","models=GridSeachCV(model,param_grid=prams,verbose=10,scoring='accuracy',n_jobs=-1,cv=5)\n","\n","\n","import optuna\n","study=optuna.create_study(direction='minimize')\n","study.optimize(optim,n_trials=15)\n","optm=partial(X=x,y=y)\n","\n","def optim(trails,x,y):\n","  entropy=trial.suggest_categorical('criterion',['gini','entropy'])\n","n_estimators=trial.sugest_int('n_estimators',100,600)\n","max_depth=trial.suggest_int('max_depth',3,13,)\n","max_features=trial.suggest_uniform('max_features',0.01,1)\n","  model=ensamble.RandomForestClassifier(\n","      n_estimators=n_estimatators,\n","      max_depth=max_depth,\n","      max_features=max_fetures,\n","      criterion=criterion\n","  )\n","  kf=model_selection.StratifiedKFold(n_splits=5)\n","  acc=[]\n","  for idx in kf.split(X=x,y=y):\n","    train_idx,test_idx=idx[0],idx[1]\n","    xtrain=x[train_idx]\n","    ytrain=y[train_idx]\n","\n","    xtest=x[test_idx]\n","    ytest=y[test_idx]\n","\n","    model.fit(xtrain,ytrian)\n","    preds=model.predict(xtest)\n","    fold_acc =metrices.accuracy_score[ytest,preds]\n","    acc.append{fold_acc}\n","  return -1.0*np.mean(acc)#-1 not needed for log losses"]},{"cell_type":"code","source":["#Lgbregressor-\n","#1.forms a historgrams when diving a column into section called bin during splitting\n","#2.combined exclusive colums into one to form a special column\n","#3.goss(gradient based one side sampling)-takes  20% records that showing largest error when classfied which then combined with 0.1% of random sample\n","#to form new sample which could be used to a gradient"],"metadata":{"id":"W8kA0JW51BBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#crossvalidation\n","\n","\n","#gridseachcv\n","#optuna\n","#hyperopt\n","#sc\n","#skopt(bayes optmization)\n","\n","#stacking\n","\n","#pipline\n","\n","\n","#tpe"],"metadata":{"id":"w1pvTuMZNvTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","from xgboost import XGBRegressor\n","\n","XGBClassifier(\n","    booster,# gbtree: tree-based models | gblinear: linear models\n","    n_estimaters=45,\n","    silent,#Silent mode is activated is set to 1, no running messages will be printed.\n","    nthread,#This is used for parallel processing, and the number of cores in the system should be entered,If you wish to run on all cores, the value should not be entered\n","\n","    eta=0.01,#(learning rate)analouse to learning rate 0.01-0.02\n","    min_child_weight=1,#minimum sum of weights of observations reqouired in a child\n","    max_depth=5,#max depth of tree 3-10,4-6\n","    max_leaf_nodes=6,#max number of leaves\n","    gamma=0,#(.1-.2)specifies minimum loss reduction required in each split\n","    max_delta_step,#usually not need constraines wieght of each tree\n","    subsample=0.75,#(.5-.9)fractions of abservation to be random sample for each tree\n","    colsample_bytree=0.75,#(.5-1)Denotes the fraction of columns to be random samples for each tree.\n","    colsample_bylevel#(usually not used)Denotes the subsample ratio of columns for each split in each level.\n","    lambda=.1,#(reg_lambda)L2 regularization term on weights\n","    alpha=.1,#(reg_aplha)L1 regularization term on weigh\n","    scale_pos_weight,#(>0)Used in case of high-class imbalance for faster convergence.\n","    objective =\"binary:logistic\",#This defines the loss function to be minimized|binary: logistic|multi: softmax|multi: softprob|\n","    eval_metric,#The evaluation metrics are to be used for validation data.\n","    seed=27,#used for generating reproducible results and also for parameter tuning\n",")\n","\n","\n","\n","\n"],"metadata":{"id":"_lfHUIyKgQJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","model=XGBRegressor(\n","     learning_rate =0.1, max_depth=4,\n"," min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n"," objective= 'multi: softmax', scale_pos_weight=1,seed=27)\n","\n","param={\n","    'min_child_weight':range(1,6,2),\n","    'max_depth':[4,5,6],\n","    'gamma':[(i/10.0) for i in range(0,5)],\n","    #'max_leaf_nodes':\n","    'subsample':[i/10.0 for i in range(6,10)],\n","    'colsample_bytree':[i/10.0 for i in range(6,10)],\n","    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n","}\n","\n","\n","#using gridsearchcv\n","models= GridSearchCV(estimator=XGBRegressor(\n","     learning_rate =0.1, max_depth=4,\n"," min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n"," objective= 'multi: softmax', scale_pos_weight=1,seed=27), param_grid = param, scoring='roc_auc',verbose=1)\n","\n","#using bayessearchcv\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","models = BayesSearchCV(estimator=model, search_spaces=params, n_jobs=-1, cv=cv)\n","\n","\n","models.fit(xtr,ytr)\n","display(pd.DataFrame(models.cv_results_))\n","\n","\n"],"metadata":{"id":"5xlQsYG7RcNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#manually coding bayessearchcv\n","from skopt.space import Integer\n","from skopt.space import Real\n","from skopt.space import Categorical\n","from skopt.utils import use_named_args\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","\n","from numpy import mean\n","param_space=[\n","    Integer(100,600,name='n_estimators'),\n","    Integer(1,6,name='min_child_weight'),\n","    Integer(3,10,name='max_depth'),\n","    Real(0,0.5,name='gamma'),\n","    Real(0.5,1,name='subsample'),\n","    Real(0.5,1,name='colsample_bytree')\n","    Real(1e-5,100,name='reg_aplha')\n","\n","]\n","\n","@use_named_args(param_space)\n","def optim(**params):\n"," model = SVC()\n"," model.set_params(**params)\n"," cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n"," result = cross_val_score(model, X, y, cv=cv, n_jobs=-1, scoring='accuracy')\n"," estimate = mean(result)\n"," return 1.0 - estimate\n","result = gp_minimize(optim, param_space)\n","# summarizing finding:\n","print('Best Accuracy: %.3f' % (1.0 - result.fun))\n","print('Best Parameters: %s' % (result.x))\n"],"metadata":{"id":"yzgEdHr776UM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from hyperopt import fmin, tpe, hp,Trials\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n","from hyperopt.pyll.base import scope\n","\n","\n","param_space={\n","\n","    \"min_child_weight\":hp.randint('min_child_weight',1,6),\n","    \"max_depth\" :hp.randint('max_depth',3,10),\n","    \"gamma\":hp.uniform('gamma'0,0.5),\n","    \"subsample\":hp.uniform('subsample',0.5,1),\n","   \"colsample_bytree\":hp.uniform('colsample_bytree',0.5,1),\n","   \"reg_aplha\":hp.uniform('reg_aplha',1e-5,100)\n","\n","}\n","\n","def optim(params):\n","  model=model\n","  acc = cross_val_score(model, xtr,ytr,scoring=\"accuracy\").mean()\n","  return {\"loss\": -acc, \"status\": STATUS_OK}\n","\n","trials=Trials()\n","best=fmin(\n","    fn=optim,\n","    space = param_space,\n","    algo=tpe.suggest,\n","    max_evals=100,\n","    trials=trials\n",")\n","\n","\n","print(\"Best: {}\".format(best))\n","display(trials.results)\n","# trials.losses(),\n","# trials.statuses()"],"metadata":{"id":"WYEKErDLNQXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from hpsklearn import HyperoptEstimator\n","from hpsklearn import any_classifier\n","from hpsklearn import any_preprocessing\n","from hpsklearn import any_regressor\n","\n","#for classifier model\n","model = HyperoptEstimator(classifier=any_classifier('cla'), preprocessing=any_preprocessing('pre'), algo=tpe.suggest, max_evals=50, trial_timeout=30)\n","#for regression model\n","model = HyperoptEstimator(regressor=any_regressor('reg'), preprocessing=any_preprocessing('pre'), loss_fn=mean_absolute_error, algo=tpe.suggest, max_evals=50, trial_timeout=30)\n","model.fit(xtr,ytr)\n","acc = model.score(xte,yte)\n","print(\"Accuracy: %.3f\" % acc)\n","# summarize the best model\n","print(model.best_model())"],"metadata":{"id":"AQS7hxlEmaKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_columns=[\"min_child_weight\",\"max_depth\",]\n"],"metadata":{"id":"1OLxe5Ywt7k8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","import sklearn\n","from sklearn.model_selection import cross_val_score\n","\n","def optim(trial):\n","\n","\n","\n"],"metadata":{"id":"2_jK-rXWDuOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import lightgbm as lgb\n","\n","model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n","model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],\n","          verbose=20,eval_metric='logloss')\n","\n","lgb.plot_tree(model,figsize=(30,40))\n","\n","# there are three type : gbdt,dart ,goss\n"],"metadata":{"id":"esSJzNrPX39K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import neptunecontrib.monitoring.skopt as sk_utils\n","import lightgbm as lgb\n","import pandas as pd\n","import neptune\n","import skopt\n","import sys\n","import os\n","\n","SEARCH_PARAMS = {'learning_rate': 0.4,\n","                'max_depth': 15,\n","                'num_leaves': 32,\n","                'feature_fraction': 0.8,\n","                'subsample': 0.2}\n","\n","FIXED_PARAMS={'objective': 'binary',\n","             'metric': 'auc',\n","             'is_unbalance':True,\n","             'bagging_freq':5,\n","             'boosting':'dart',\n","             'num_boost_round':300,\n","             'early_stopping_rounds':30}\n","\n","def train_evaluate(search_params):\n","\n","\n","\n","\n","   params = {'metric':FIXED_PARAMS['metric'],\n","             'objective':FIXED_PARAMS['objective'],\n","             **search_params}\n","\n","   model = lgb.train(params, train_data,\n","                     valid_sets=[test_data],\n","                     num_boost_round=FIXED_PARAMS['num_boost_round'],\n","                     early_stopping_rounds=FIXED_PARAMS['early_stopping_rounds'],\n","                     valid_names=['valid'])\n","   score = model.best_score['valid']['auc']\n","   return score"],"metadata":{"id":"oacftCvZahnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lgb.plot_metric(model)"],"metadata":{"id":"wTa9G331YWP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics.plot_confusion_matrix(model,x_test,y_test,cmap='Blues_r')"],"metadata":{"id":"LNP8UBJDYZCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZsUz0PyihTrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5msa6PVJhUQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from catboost import CatBoostClassifier,Pool\n","from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report\n","\n","cat_var = np.where(xtr.dtypes != np.float)[0]\n","train_pool = Pool(data=xtr, label=y_train, cat_features=cat_var, feature_names=feature_names)\n","test_pool = Pool(data=xte, label=y_test, cat_features=cat_var, feature_names=feature_names)\n","#cat_featurers=catogrical featuers\n","# Define CatBoost parameters\n","params = {\n","    'iterations': 100,\n","    'depth': 6,\n","    'learning_rate': 0.1,\n","    'loss_function': 'Logloss',  # Classification task\n","    'custom_metric': ['Accuracy', 'AUC'],  # Additional metrics to track\n","    'verbose': 10,  # Print training progress every 10 iterations\n","    'random_seed': 42  # Set a random seed for reproducibility\n","}\n","\n","model = CatBoostClassifier(**params)\n","model.fit(train_pool, eval_set=test_pool)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(test_pool)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","logloss = log_loss(y_test, model.predict_proba(test_pool)[:, 1])\n","roc_auc = roc_auc_score(y_test, model.predict_proba(test_pool)[:, 1])\n","\n","# Print evaluation metrics\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Log Loss: {logloss:.4f}\")\n","print(f\"AUC: {roc_auc:.4f}\")\n","\n","class_report = classification_report(y_test, y_pred)\n","print(\"Classification Report:\\n\", class_report)"],"metadata":{"id":"exwCDZ50_o75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import catboost as cb\n","from sklearn.metrics import mean_squared_error\n","import optuna\n","\n","def objective(trial):\n","    params = {\n","        \"iterations\": 1000,\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n","        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n","        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n","        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n","    }\n","\n","    model = cb.CatBoostRegressor(**params, silent=True)\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_val)\n","    rmse = mean_squared_error(y_val, predictions, squared=False)\n","    return rmse\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=30)\n","print('Best hyperparameters:', study.best_params)\n","print('Best RMSE:', study.best_value)\n"],"metadata":{"id":"awa1DDc7k55K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=CatBoostClassifier(\n","    iterations=,\n","    learning_rate,\n","    depth=\n",")"],"metadata":{"id":"HnQ0HxC1fEej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import numpy as np\n","import optuna\n","from optuna.integration import CatBoostPruningCallback\n","\n","import catboost as cb\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","\n","def objective(trial: optuna.Trial) -> float:\n","\n","    param = {\n","        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n","        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n","        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n","        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n","        \"bootstrap_type\": trial.suggest_categorical(\n","            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n","        ),\n","        \"used_ram_limit\": \"3gb\",\n","        \"eval_metric\": \"Accuracy\",\n","    }\n","\n","    if param[\"bootstrap_type\"] == \"Bayesian\":\n","        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n","    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n","        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n","\n","    gbm = cb.CatBoostClassifier(**param)\n","\n","    pruning_callback = CatBoostPruningCallback(trial, \"Accuracy\")\n","    gbm.fit(\n","        train_x,\n","        train_y,\n","        eval_set=[(valid_x, valid_y)],\n","        verbose=0,\n","        early_stopping_rounds=100,\n","        callbacks=[pruning_callback],\n","    )\n","\n","    # evoke pruning manually.\n","    pruning_callback.check_pruned()\n","\n","    preds = gbm.predict(valid_x)\n","    pred_labels = np.rint(preds)\n","    accuracy = accuracy_score(valid_y, pred_labels)\n","\n","    return accuracy\n","\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(\n","        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n","    )\n","    study.optimize(objective, n_trials=100, timeout=600)\n","\n","    print(\"Number of finished trials: {}\".format(len(study.trials)))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"],"metadata":{"id":"MqWRXJPgCEAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Optuna example that optimizes a classifier configuration for cancer dataset using LightGBM.\n","\n","In this example, we optimize the validation accuracy of cancer detection using LightGBM.\n","We optimize both the choice of booster model and their hyperparameters.\n","\n","\"\"\"\n","\n","import numpy as np\n","import optuna\n","\n","import lightgbm as lgb\n","import sklearn.datasets\n","import sklearn.metrics\n","from sklearn.model_selection import train_test_split\n","\n","\n","# FYI: Objective functions can take additional arguments\n","# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n","def objective(trial):\n","    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n","    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n","    dtrain = lgb.Dataset(train_x, label=train_y)\n","\n","    param = {\n","        \"objective\": \"binary\",\n","        \"metric\": \"binary_logloss\",\n","        \"verbosity\": -1,\n","        \"boosting_type\": \"gbdt\",\n","        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n","        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n","        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n","        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n","        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n","        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n","    }\n","\n","    gbm = lgb.train(param, dtrain)\n","    preds = gbm.predict(valid_x)\n","    pred_labels = np.rint(preds)\n","    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n","    return accuracy\n","\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(objective, n_trials=100)\n","\n","    print(\"Number of finished trials: {}\".format(len(study.trials)))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"],"metadata":{"id":"YwPTj6xdltO5"},"execution_count":null,"outputs":[]}]}