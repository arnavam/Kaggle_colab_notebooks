{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ3OnJFElzVx"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorsizer =CountVectorizer()\n","xtr_count=vectorizer.fit_transform(xtr.values)\n","\n","\n","print(vectorizer.get_feature_names())\n","print(xtr_count.toarray())\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0woZZ32I29AW"},"outputs":[],"source":["from sklearn.preprocessing import labelEncoder\n","\n","le_column1=labelEncoder()\n","le_column2=labelEncoder()\n","...\n","\n","x['col1_n'] =le_column1.fit_tranform(x['col1'])\n","x['col2_n']=le_column2.fit_transform(x['col1'])\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8PoP_YRAIwu"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scalar=MinMaxScaler()\n","\n","scalar.fit(x['col1'])\n","x['col1']=scalar.transform(x['col1'])\n","scalar.fit(x['col2'])\n","x['col2']=scalar.transform(x['col2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMaKZNaHbtpn"},"outputs":[],"source":["from sklearn.preprocessing import StandardScalar\n","\n","scalar =StandardScalar()\n","x_scaled=scalar.fit_tranform(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39dUhvEz-pIk"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","\n","k_rng=10\n","sse=[]\n","cen=[]\n","for k in range(k_rng):\n","  km=KMeans(n_clusters=k)\n","  pre=km.fit_predit(x)\n","  cen.append(km.cluster_centers_)\n","  sse.append(km.inertia_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc-5ttbHxg9I"},"outputs":[],"source":["from sklearn .model_selection import train_test_split\n","\n","xtr,xte,ytr,yte=train_test_split(x,y,test_size=0.2,shuffle=True,stratify=y,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlpogICI2EMR"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","kf= KFold(n_splits=3)\n","\n","\n","for tr_i ,te_i in kf.split(list):\n","  print(tr_i,te_i)\n","\n","from sklearn.model_selection import StratifiedKFold\n","folds=StratifiedKFold(n_splits=0)\n","\n","for tr_i ,te_i in kf.split(x):\n","  xtr,xte,ytr,yte=x[tr_i],x[te_i],\\\n","                  y[tr_i],y[te_i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEuyJgiYyT0E"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","model=LogisticRegression()\n","\n","\n","from sklearn.linear_model import Lasso\n","model=Lasso(alpha=5-,max_iter=100,tol=0.1)#uses L1 regularization ,adds aplha*mod of bias term to loss\n","\n","from sklern.linear_model import Ridge\n","model=Ridge(alpha=50,max_iter=100,tol=0.1)#uses L2 regularization ,add alpa*bias^2 term to loss\n","\n","from sklearn.tree import DecisionTreeClassifier\n","model=DecisionTreeClassifier()\n","\n","\n","\n","from sklearn.svm import SVC\n","model=SVC(C=10,gamma=1,kernel='linear')\n","\n","from sklearn.ensemble import RandomForestClassifier\n","model=RandomForestClassifier(n_estimators=40)\n","\n","from sklearn.naive_bayes import GaussianNB\n","model=GaussianNB()#the one based on probablities\n","\n","\n","from sklearn.naive_bayes import MultinomialNB\n","model=MultinomialNB()\n","#bernouli Naive Bayes -0s and 1s\n","#multinomial naive bayes- counting numbers\n","#gausiian Naive bayes -numbers are decimals,data is contnious,when there is bell curve\n","\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","model=KNeighbourClassifier(n_neighbors=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r55ib-17M_LL"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","cross_val_score(model,x,y)\n","#alternate method from score and fit ,k folding\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9AkxX3pQylfA","outputId":"0ae81c18-6d4f-4172-bea4-f4044f848e6e"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-da78758df397>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.fit(xtr,ytr)\n","model.score(xte,yte)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FHXtsgSNzIX"},"outputs":[],"source":["model.predict(x[0])\n","ypr=model.predict(xte)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nlSderRlPpu"},"outputs":[],"source":["df.groupby('Label').describe()"]},{"cell_type":"markdown","metadata":{"id":"uJaIGvd7zNgR"},"source":["m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wjWuoSIzhsm"},"outputs":[],"source":["from sklearn.metrics import confusion_metrix\n","cm = confusion_metrix(yte ,pre)\n","cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dCUrkVx0H2Z"},"outputs":[],"source":["import seaborn as sn\n","\n","plt.figure(figsize= (10,7))\n","sn.heatmap(cm,annot=True)\n","plt.xlabel('predicted')\n","plt.ylabel('Truth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSEG6OO6PzVo"},"outputs":[],"source":["from sklearn.metrcies import classification_report\n","print(classfication_report(yt,pre))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyBZOfHUpdc_"},"outputs":[],"source":["from sklearn.pipline import Pipeline\n","\n","models=Pipeline([\n","    ('vectorizer',CountVectorizer()),\n","    ('nb',MUltinomialNB())\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9AEUQNC5SSf"},"outputs":[],"source":["#hypertuning - tuning your model to give best result\n","#hyperplane - plane that divides data in svm\n","\n","form sklearn.model_selectin import GridSearchV\n","models = GridSearcCV(svm.SVC(gama='auto'),{\n","    'C':[1,10,20],\n","    'kernel':['rbf','linear']\n","},cv=5,return_trian_score=False)\n","#has high computational cost  to reduce it\n","\n","\n","form sklearn.model_selectin import randomizedSearchCV\n","models = RandomizedSearcCV(svm.SVC(gama='auto'),{\n","    'C':[1,10,20],\n","    'kernel':['rbf','linear']\n","},cv=5,return_trian_score=False, n_iter=2)\n","\n","models.fit(x,y)\n","display(pd.DataFrame(models.cv_results_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT0Bzi8fuLGY"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassfier\n","models = BagginClassifier(\n","    base_estimator=DecisionTreeClassifier(),\n","    n_estimators=100,\n","    max_samples=0.8,\n","    oob_score=True ,#oob-out of the bag -data thats miss due to  random sampling is used for test\n","    random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp63z-dcaOVb"},"outputs":[],"source":["#calcualate new featuers which covers most varience\n","#first a new x and y are drawn for each corrseponding featuers to make the data closer to the center\n","#second reduce no of featuers\n","\n","from sklearn.decompostion import PCA\n","pca=PCA(0.95)\n","x_pca=pca.fit_tranform(x)"]},{"cell_type":"code","source":["#some times random subsampling can make test and train not having equal divide between class\n","#standardization used instead of minmax during mean and sigma should be of train_data\n","#sample vs population standard deviation\n","from sklearn.preprocessing import StandardScaler\n","\n","scalar=StandardScalar()\n","scalar.fit(x)\n","x_std=scalar.transform(x)"],"metadata":{"id":"XoiX11e2JKfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","from sklearn.evaluvate import predifinedHoldoutSplit\n","\n","pipe=make_pipeline(StandardScalar(),KNeighborsClassifier(n_neighbors=3))\n","\n","params={'kneighborsclassifier_n_negihbors':[1,3,5],'kneighborsclassifier_p':[1,2]}\n","split=PredefinedHOldoutSplit(valid_indices=y)\n","\n","grid=GridSearchCV(pipe,\n","                  param_grid=params,\n","                  cv=split)\n","\n","grid.best_score_\n","grid.best_params_\n","for i,j in zip(grid.cv_results_['params'],grid.cv_results_['mean_test_score']):\n","  print(i,j)"],"metadata":{"id":"EixLxcIvP6YZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#boosting techniques\n","#1. ada boost -desciion trees are first created called stumps which only have one split,the record they misclassfied are then passed to another descion tree which again classify it\n","#each reacord is given a wieght and the total error and perforamnce of stump is calculated using misclassfied records weight\n","#incorrectly misclassfied are then multiplied by e^perfomance correctly classfied are then multiplied by e^-perfomance,\n","#wieghts ae then normalised by dividing by the sum of new weights\n","#these new weifghs and then made into intervals  and the misclassified one have largest intervals since they have largest weight\n","#for the next descision tree a new records must be choosen for that  they choose a number between 0 to 1 and find under which weight interval they fall into and that records are choosen\n","#this is continued m times\n","#for the test data its run by m+1 the stumps and mean of there output are then taken as final output\n","\n","#2.gradien boost-similar to adaboost\n","#base record created by assign mean of all record\n","# residual recoard is created using on a loss function which calucalte how much loss there is between orginal record and base record\n","#thsi is give as ouput for a deciscion tree\n","#output of decison tree is multiple by alpha(learning rate )then added to base record\n","#another decsion tree is created based this new record whose output then again multplied and added to previous record\n","#this is done n times"],"metadata":{"id":"KjF6kPN5msha"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMR55ZpBM0mytRnEa54LwFw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}